{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojW8pWR2dJgi"
   },
   "source": [
    "# **💡0️⃣ [ 아래 코드를 실행하여 조별 번호를 입력하시오.]💡**\n",
    "\n",
    "*   1조: 박유진, 김서연, 최진\n",
    "*   2조: 서민경, 이유진, 최범영\n",
    "*   3조: 정은주, 김장환, 권진경\n",
    "*   4조: 정진교, 장채은, 조성환\n",
    "*   5조: 전승재, 신은호, 김윤희\n",
    "*   6조: 정현서, 진경은, 이정민\n",
    "*   7조: 이성지, 김내경\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_aMFjTcbntUh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[예]조의 번호를 입력하시오: 1\n",
      ">> 5조 입니다.\n"
     ]
    }
   ],
   "source": [
    "print(\"[예]조의 번호를 입력하시오: 1\")\n",
    "team_idx = int(input(\">> 조의 번호를 입력하시오: \"))\n",
    "print(f\">> {team_idx}조 입니다.\")\n",
    "team_idx = 'team'+str(team_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ltFiLYoDzdu9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.datasets.utils import download_url, extract_archive\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "from torchinfo import summary\n",
    "\n",
    "URL = \"https://github.com/JanghunHyeon/AISW4202-Project/releases/download/v.1.1.0/project_dataset.zip\"\n",
    "ROOT = \"./content/data\"\n",
    "ZIP_PATH = os.path.join(ROOT, \"project_dataset.zip\")\n",
    "OUT_DIR  = os.path.join(ROOT, \"project_dataset\")\n",
    "\n",
    "os.makedirs(ROOT, exist_ok=True)\n",
    "download_url(URL, root=ROOT, filename=\"project_dataset.zip\")\n",
    "extract_archive(ZIP_PATH, OUT_DIR)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# Reproduce를 위한 Seed 고정\n",
    "seed_id = 777\n",
    "deterministic = True\n",
    "\n",
    "random.seed(seed_id)\n",
    "np.random.seed(seed_id)\n",
    "torch.manual_seed(seed_id)\n",
    "if device =='cuda':\n",
    "    torch.cuda.manual_seed_all(seed_id)\n",
    "if deterministic:\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\ttorch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owZuDayDQfCI"
   },
   "source": [
    "# **💡1️⃣ [ Design your custom model ]💡**\n",
    "  ## 아래 코드를 수정하여 본인의 모델을 만들어 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kA09kjRQRJm_"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 1. 블록 정의\n",
    "############################\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, c_in, c_out, stride=1):\n",
    "        super().__init__()\n",
    "        self.depth = nn.Conv2d(c_in, c_in, 3, stride, 1, groups=c_in, bias=False)\n",
    "        self.point = nn.Conv2d(c_in, c_out, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c_out)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn(self.point(self.depth(x))))\n",
    "        return x\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, ch, r=4):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(ch, ch // r, bias=False)\n",
    "        self.fc2 = nn.Linear(ch // r, ch, bias=False)\n",
    "        self.act = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = x.mean((2,3))                    # Squeeze\n",
    "        s = self.sig(self.fc2(self.act(self.fc1(s))))\n",
    "        s = s.view(s.size(0), -1, 1, 1)\n",
    "        return x * s                         # Excitation\n",
    "\n",
    "class DwResSEBlock(nn.Module):\n",
    "    def __init__(self, c_in, c_out, stride):\n",
    "        super().__init__()\n",
    "        self.conv = DepthwiseSeparableConv(c_in, c_out, stride)\n",
    "        self.se   = SEBlock(c_out)\n",
    "        self.skip = nn.Sequential()\n",
    "        if stride != 1 or c_in != c_out:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(c_in, c_out, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(c_out))\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.se(self.conv(x))\n",
    "        out = out + self.skip(x)\n",
    "        return self.act(out)\n",
    "\n",
    "############################\n",
    "# 2. MyCustomModel 교체\n",
    "############################\n",
    "class MyCustomModel(nn.Module):\n",
    "    def __init__(self, num_classes=15):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.stage1 = self._make_layer(32,  64,  2, stride=2) # 48→24\n",
    "        self.stage2 = self._make_layer(64,  128, 2, stride=2) # 24→12\n",
    "        self.stage3 = self._make_layer(128, 256, 3, stride=2) # 12→6\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc   = nn.Linear(256, num_classes)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, c_in, c_out, blocks, stride):\n",
    "        layers = [DwResSEBlock(c_in, c_out, stride)]\n",
    "        layers += [DwResSEBlock(c_out, c_out, 1) for _ in range(blocks-1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.pool(x).view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "HiQdYRtCRLtY"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "out_channels must be divisible by groups",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mMyCustomModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m      3\u001b[39m inputs = torch.Tensor(\u001b[32m1\u001b[39m,\u001b[32m3\u001b[39m,\u001b[32m48\u001b[39m,\u001b[32m48\u001b[39m).to(device)\n\u001b[32m      4\u001b[39m out = model(inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 265\u001b[39m, in \u001b[36mMyCustomModel.__init__\u001b[39m\u001b[34m(self, num_classes, init_weights)\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[38;5;28mself\u001b[39m.res_block2 = \u001b[38;5;28mself\u001b[39m._make_residual_block(\u001b[32m512\u001b[39m, \u001b[32m512\u001b[39m)\n\u001b[32m    263\u001b[39m \u001b[38;5;66;03m# Exit flow - Final feature extraction\u001b[39;00m\n\u001b[32m    264\u001b[39m \u001b[38;5;28mself\u001b[39m.exit_conv = nn.Sequential(\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     \u001b[43mAdvancedSeparableConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m728\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m    266\u001b[39m     nn.ReLU(),\n\u001b[32m    267\u001b[39m     AdvancedSeparableConv2d(\u001b[32m728\u001b[39m, \u001b[32m1024\u001b[39m, kernel_size=\u001b[32m3\u001b[39m, stride=\u001b[32m1\u001b[39m, padding=\u001b[32m1\u001b[39m),\n\u001b[32m    268\u001b[39m     nn.ReLU()\n\u001b[32m    269\u001b[39m )\n\u001b[32m    271\u001b[39m \u001b[38;5;66;03m# SE block for final features\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[38;5;28mself\u001b[39m.final_se = SEBlock(\u001b[32m1024\u001b[39m, reduction=\u001b[32m32\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mAdvancedSeparableConv2d.__init__\u001b[39m\u001b[34m(self, in_channels, out_channels, kernel_size, stride, padding, use_multiple_kernels, expansion_factor)\u001b[39m\n\u001b[32m     31\u001b[39m expanded_channels = in_channels * expansion_factor\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_multiple_kernels:\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Multi-scale depthwise convolution (like MobileNetV2 inspiration)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28mself\u001b[39m.depthwise_3x3 = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_channels\u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mself\u001b[39m.depthwise_5x5 = nn.Conv2d(in_channels, expanded_channels//\u001b[32m2\u001b[39m, kernel_size=\u001b[32m5\u001b[39m, \n\u001b[32m     38\u001b[39m                                  stride=stride, padding=\u001b[32m2\u001b[39m, groups=in_channels)\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mself\u001b[39m.bn_dw = nn.BatchNorm2d(expanded_channels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/pro/lib/python3.13/site-packages/torch/nn/modules/conv.py:521\u001b[39m, in \u001b[36mConv2d.__init__\u001b[39m\u001b[34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[39m\n\u001b[32m    519\u001b[39m padding_ = padding \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m _pair(padding)\n\u001b[32m    520\u001b[39m dilation_ = _pair(dilation)\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkernel_size_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdilation_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/pro/lib/python3.13/site-packages/torch/nn/modules/conv.py:109\u001b[39m, in \u001b[36m_ConvNd.__init__\u001b[39m\u001b[34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[39m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33min_channels must be divisible by groups\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out_channels % groups != \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mout_channels must be divisible by groups\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    110\u001b[39m valid_padding_strings = {\u001b[33m\"\u001b[39m\u001b[33msame\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalid\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[31mValueError\u001b[39m: out_channels must be divisible by groups"
     ]
    }
   ],
   "source": [
    "model = MyCustomModel(num_classes=15).to(device)\n",
    "\n",
    "inputs = torch.Tensor(1,3,48,48).to(device)\n",
    "out = model(inputs)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "iGFrmqopTjku"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "MyCustomModel                                 [1, 15]                   --\n",
       "├─Sequential: 1-1                             [1, 32, 48, 48]           --\n",
       "│    └─Conv2d: 2-1                            [1, 32, 48, 48]           864\n",
       "│    └─BatchNorm2d: 2-2                       [1, 32, 48, 48]           64\n",
       "│    └─ReLU: 2-3                              [1, 32, 48, 48]           --\n",
       "├─Sequential: 1-2                             [1, 64, 24, 24]           --\n",
       "│    └─DwResSEBlock: 2-4                      [1, 64, 24, 24]           --\n",
       "│    │    └─DepthwiseSeparableConv: 3-1       [1, 64, 24, 24]           2,464\n",
       "│    │    └─SEBlock: 3-2                      [1, 64, 24, 24]           2,048\n",
       "│    │    └─Sequential: 3-3                   [1, 64, 24, 24]           2,176\n",
       "│    │    └─ReLU: 3-4                         [1, 64, 24, 24]           --\n",
       "│    └─DwResSEBlock: 2-5                      [1, 64, 24, 24]           --\n",
       "│    │    └─DepthwiseSeparableConv: 3-5       [1, 64, 24, 24]           4,800\n",
       "│    │    └─SEBlock: 3-6                      [1, 64, 24, 24]           2,048\n",
       "│    │    └─Sequential: 3-7                   [1, 64, 24, 24]           --\n",
       "│    │    └─ReLU: 3-8                         [1, 64, 24, 24]           --\n",
       "├─Sequential: 1-3                             [1, 128, 12, 12]          --\n",
       "│    └─DwResSEBlock: 2-6                      [1, 128, 12, 12]          --\n",
       "│    │    └─DepthwiseSeparableConv: 3-9       [1, 128, 12, 12]          9,024\n",
       "│    │    └─SEBlock: 3-10                     [1, 128, 12, 12]          8,192\n",
       "│    │    └─Sequential: 3-11                  [1, 128, 12, 12]          8,448\n",
       "│    │    └─ReLU: 3-12                        [1, 128, 12, 12]          --\n",
       "│    └─DwResSEBlock: 2-7                      [1, 128, 12, 12]          --\n",
       "│    │    └─DepthwiseSeparableConv: 3-13      [1, 128, 12, 12]          17,792\n",
       "│    │    └─SEBlock: 3-14                     [1, 128, 12, 12]          8,192\n",
       "│    │    └─Sequential: 3-15                  [1, 128, 12, 12]          --\n",
       "│    │    └─ReLU: 3-16                        [1, 128, 12, 12]          --\n",
       "├─Sequential: 1-4                             [1, 256, 6, 6]            --\n",
       "│    └─DwResSEBlock: 2-8                      [1, 256, 6, 6]            --\n",
       "│    │    └─DepthwiseSeparableConv: 3-17      [1, 256, 6, 6]            34,432\n",
       "│    │    └─SEBlock: 3-18                     [1, 256, 6, 6]            32,768\n",
       "│    │    └─Sequential: 3-19                  [1, 256, 6, 6]            33,280\n",
       "│    │    └─ReLU: 3-20                        [1, 256, 6, 6]            --\n",
       "│    └─DwResSEBlock: 2-9                      [1, 256, 6, 6]            --\n",
       "│    │    └─DepthwiseSeparableConv: 3-21      [1, 256, 6, 6]            68,352\n",
       "│    │    └─SEBlock: 3-22                     [1, 256, 6, 6]            32,768\n",
       "│    │    └─Sequential: 3-23                  [1, 256, 6, 6]            --\n",
       "│    │    └─ReLU: 3-24                        [1, 256, 6, 6]            --\n",
       "│    └─DwResSEBlock: 2-10                     [1, 256, 6, 6]            --\n",
       "│    │    └─DepthwiseSeparableConv: 3-25      [1, 256, 6, 6]            68,352\n",
       "│    │    └─SEBlock: 3-26                     [1, 256, 6, 6]            32,768\n",
       "│    │    └─Sequential: 3-27                  [1, 256, 6, 6]            --\n",
       "│    │    └─ReLU: 3-28                        [1, 256, 6, 6]            --\n",
       "├─AdaptiveAvgPool2d: 1-5                      [1, 256, 1, 1]            --\n",
       "├─Linear: 1-6                                 [1, 15]                   3,855\n",
       "===============================================================================================\n",
       "Total params: 372,687\n",
       "Trainable params: 372,687\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 19.59\n",
       "===============================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 5.28\n",
       "Params size (MB): 1.49\n",
       "Estimated Total Size (MB): 6.80\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, (1, 3, 48, 48))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4870zoF_T52f"
   },
   "source": [
    "# **💡2️⃣[ Dataset Processing ]💡**\n",
    "\n",
    "## Dataset for Final Project\n",
    "- Large-scale RGB image dataset\n",
    "  - Training set: about 75,000 images\n",
    "  - Validation set: 900 images\n",
    "  \n",
    "- Dataset specification\n",
    "  - image size: 48 x 48\n",
    "  - RGB scale image (3 channel)\n",
    "  - 15 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "53C2-8_TTqOG"
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std =[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std =[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "data_dir = os.path.join(OUT_DIR, \"data/ProjectDataset\")\n",
    "trainset = ImageFolder(os.path.join(data_dir, \"train\"), transform=transform_train)\n",
    "valset = ImageFolder(os.path.join(data_dir, \"val\"), transform=transform_val)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=256, shuffle=True, num_workers=6)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omABX5cXW67v"
   },
   "source": [
    "# **💡3️⃣[ Train your custom model ]💡**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "gH7IQXB8T_LH"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
    "\n",
    "lr_sche = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "oMb8Pr7TXBVH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n",
      "[1,    50] loss: 2.017\n",
      "[1,   100] loss: 1.676\n",
      "[1,   150] loss: 1.540\n",
      "[1,   200] loss: 1.429\n",
      "[1,   250] loss: 1.348\n",
      "Accuracy of the network on the 1,200 validation images: 6.667 %\n",
      "[2,    50] loss: 1.211\n",
      "[2,   100] loss: 1.156\n",
      "[2,   150] loss: 1.127\n",
      "[2,   200] loss: 1.098\n",
      "[2,   250] loss: 1.055\n",
      "Accuracy of the network on the 1,200 validation images: 6.667 %\n",
      "[3,    50] loss: 0.995\n",
      "[3,   100] loss: 0.970\n",
      "[3,   150] loss: 0.961\n",
      "[3,   200] loss: 0.953\n",
      "[3,   250] loss: 0.938\n",
      "Accuracy of the network on the 1,200 validation images: 68.111 %\n",
      "[4,    50] loss: 0.898\n",
      "[4,   100] loss: 0.894\n",
      "[4,   150] loss: 0.877\n",
      "[4,   200] loss: 0.878\n",
      "[4,   250] loss: 0.863\n",
      "Accuracy of the network on the 1,200 validation images: 72.889 %\n",
      "[5,    50] loss: 0.832\n",
      "[5,   100] loss: 0.808\n",
      "[5,   150] loss: 0.801\n",
      "[5,   200] loss: 0.807\n",
      "[5,   250] loss: 0.805\n",
      "Accuracy of the network on the 1,200 validation images: 73.222 %\n",
      "[6,    50] loss: 0.749\n",
      "[6,   100] loss: 0.732\n",
      "[6,   150] loss: 0.747\n",
      "[6,   200] loss: 0.749\n",
      "[6,   250] loss: 0.758\n",
      "Accuracy of the network on the 1,200 validation images: 74.000 %\n",
      "[7,    50] loss: 0.703\n",
      "[7,   100] loss: 0.692\n",
      "[7,   150] loss: 0.716\n",
      "[7,   200] loss: 0.728\n",
      "[7,   250] loss: 0.718\n",
      "Accuracy of the network on the 1,200 validation images: 74.333 %\n",
      "[8,    50] loss: 0.659\n",
      "[8,   100] loss: 0.686\n",
      "[8,   150] loss: 0.673\n",
      "[8,   200] loss: 0.686\n",
      "[8,   250] loss: 0.683\n",
      "Accuracy of the network on the 1,200 validation images: 76.111 %\n",
      "[9,    50] loss: 0.651\n",
      "[9,   100] loss: 0.633\n",
      "[9,   150] loss: 0.665\n",
      "[9,   200] loss: 0.655\n",
      "[9,   250] loss: 0.662\n",
      "Accuracy of the network on the 1,200 validation images: 75.556 %\n",
      "[10,    50] loss: 0.609\n",
      "[10,   100] loss: 0.614\n",
      "[10,   150] loss: 0.610\n",
      "[10,   200] loss: 0.622\n",
      "[10,   250] loss: 0.639\n",
      "Accuracy of the network on the 1,200 validation images: 76.778 %\n",
      "[11,    50] loss: 0.573\n",
      "[11,   100] loss: 0.582\n",
      "[11,   150] loss: 0.580\n",
      "[11,   200] loss: 0.591\n",
      "[11,   250] loss: 0.598\n",
      "Accuracy of the network on the 1,200 validation images: 76.111 %\n",
      "[12,    50] loss: 0.538\n",
      "[12,   100] loss: 0.556\n",
      "[12,   150] loss: 0.579\n",
      "[12,   200] loss: 0.576\n",
      "[12,   250] loss: 0.582\n",
      "Accuracy of the network on the 1,200 validation images: 77.444 %\n",
      "[13,    50] loss: 0.545\n",
      "[13,   100] loss: 0.518\n",
      "[13,   150] loss: 0.552\n",
      "[13,   200] loss: 0.553\n",
      "[13,   250] loss: 0.561\n",
      "Accuracy of the network on the 1,200 validation images: 77.000 %\n",
      "[14,    50] loss: 0.505\n",
      "[14,   100] loss: 0.523\n",
      "[14,   150] loss: 0.538\n",
      "[14,   200] loss: 0.528\n",
      "[14,   250] loss: 0.531\n",
      "Accuracy of the network on the 1,200 validation images: 77.444 %\n",
      "[15,    50] loss: 0.486\n",
      "[15,   100] loss: 0.498\n",
      "[15,   150] loss: 0.518\n",
      "[15,   200] loss: 0.494\n",
      "[15,   250] loss: 0.523\n",
      "Accuracy of the network on the 1,200 validation images: 78.111 %\n",
      "[16,    50] loss: 0.451\n",
      "[16,   100] loss: 0.463\n",
      "[16,   150] loss: 0.475\n",
      "[16,   200] loss: 0.485\n",
      "[16,   250] loss: 0.477\n",
      "Accuracy of the network on the 1,200 validation images: 77.111 %\n",
      "[17,    50] loss: 0.447\n",
      "[17,   100] loss: 0.443\n",
      "[17,   150] loss: 0.448\n",
      "[17,   200] loss: 0.458\n",
      "[17,   250] loss: 0.479\n",
      "Accuracy of the network on the 1,200 validation images: 78.778 %\n",
      "[18,    50] loss: 0.430\n",
      "[18,   100] loss: 0.420\n",
      "[18,   150] loss: 0.451\n",
      "[18,   200] loss: 0.443\n",
      "[18,   250] loss: 0.447\n",
      "Accuracy of the network on the 1,200 validation images: 79.444 %\n",
      "[19,    50] loss: 0.409\n",
      "[19,   100] loss: 0.415\n",
      "[19,   150] loss: 0.422\n",
      "[19,   200] loss: 0.435\n",
      "[19,   250] loss: 0.446\n",
      "Accuracy of the network on the 1,200 validation images: 79.556 %\n",
      "[20,    50] loss: 0.369\n",
      "[20,   100] loss: 0.415\n",
      "[20,   150] loss: 0.399\n",
      "[20,   200] loss: 0.420\n",
      "[20,   250] loss: 0.434\n",
      "Accuracy of the network on the 1,200 validation images: 78.111 %\n",
      "[21,    50] loss: 0.358\n",
      "[21,   100] loss: 0.369\n",
      "[21,   150] loss: 0.379\n",
      "[21,   200] loss: 0.402\n",
      "[21,   250] loss: 0.381\n",
      "Accuracy of the network on the 1,200 validation images: 79.333 %\n",
      "[22,    50] loss: 0.343\n",
      "[22,   100] loss: 0.364\n",
      "[22,   150] loss: 0.375\n",
      "[22,   200] loss: 0.367\n",
      "[22,   250] loss: 0.383\n",
      "Accuracy of the network on the 1,200 validation images: 79.444 %\n",
      "[23,    50] loss: 0.323\n",
      "[23,   100] loss: 0.351\n",
      "[23,   150] loss: 0.341\n",
      "[23,   200] loss: 0.364\n",
      "[23,   250] loss: 0.366\n",
      "Accuracy of the network on the 1,200 validation images: 78.889 %\n",
      "[24,    50] loss: 0.317\n",
      "[24,   100] loss: 0.319\n",
      "[24,   150] loss: 0.338\n",
      "[24,   200] loss: 0.350\n",
      "[24,   250] loss: 0.354\n",
      "Accuracy of the network on the 1,200 validation images: 78.556 %\n",
      "[25,    50] loss: 0.309\n",
      "[25,   100] loss: 0.321\n",
      "[25,   150] loss: 0.332\n",
      "[25,   200] loss: 0.337\n",
      "[25,   250] loss: 0.355\n",
      "Accuracy of the network on the 1,200 validation images: 77.667 %\n",
      "[26,    50] loss: 0.288\n",
      "[26,   100] loss: 0.286\n",
      "[26,   150] loss: 0.297\n",
      "[26,   200] loss: 0.303\n",
      "[26,   250] loss: 0.322\n",
      "Accuracy of the network on the 1,200 validation images: 77.778 %\n",
      "[27,    50] loss: 0.276\n",
      "[27,   100] loss: 0.272\n",
      "[27,   150] loss: 0.278\n",
      "[27,   200] loss: 0.293\n",
      "[27,   250] loss: 0.305\n",
      "Accuracy of the network on the 1,200 validation images: 77.556 %\n",
      "[28,    50] loss: 0.266\n",
      "[28,   100] loss: 0.274\n",
      "[28,   150] loss: 0.287\n",
      "[28,   200] loss: 0.279\n",
      "[28,   250] loss: 0.298\n",
      "Accuracy of the network on the 1,200 validation images: 78.333 %\n",
      "[29,    50] loss: 0.266\n",
      "[29,   100] loss: 0.251\n",
      "[29,   150] loss: 0.270\n",
      "[29,   200] loss: 0.269\n",
      "[29,   250] loss: 0.281\n",
      "Accuracy of the network on the 1,200 validation images: 78.111 %\n",
      "[30,    50] loss: 0.237\n",
      "[30,   100] loss: 0.237\n",
      "[30,   150] loss: 0.262\n",
      "[30,   200] loss: 0.273\n",
      "[30,   250] loss: 0.267\n",
      "Accuracy of the network on the 1,200 validation images: 78.444 %\n",
      "[31,    50] loss: 0.231\n",
      "[31,   100] loss: 0.228\n",
      "[31,   150] loss: 0.244\n",
      "[31,   200] loss: 0.235\n",
      "[31,   250] loss: 0.255\n",
      "Accuracy of the network on the 1,200 validation images: 77.222 %\n",
      "[32,    50] loss: 0.217\n",
      "[32,   100] loss: 0.224\n",
      "[32,   150] loss: 0.230\n",
      "[32,   200] loss: 0.233\n",
      "[32,   250] loss: 0.234\n",
      "Accuracy of the network on the 1,200 validation images: 77.444 %\n",
      "[33,    50] loss: 0.204\n",
      "[33,   100] loss: 0.209\n",
      "[33,   150] loss: 0.214\n",
      "[33,   200] loss: 0.248\n",
      "[33,   250] loss: 0.245\n",
      "Accuracy of the network on the 1,200 validation images: 78.222 %\n",
      "[34,    50] loss: 0.209\n",
      "[34,   100] loss: 0.194\n",
      "[34,   150] loss: 0.204\n",
      "[34,   200] loss: 0.217\n",
      "[34,   250] loss: 0.213\n",
      "Accuracy of the network on the 1,200 validation images: 77.556 %\n",
      "[35,    50] loss: 0.203\n",
      "[35,   100] loss: 0.191\n",
      "[35,   150] loss: 0.204\n",
      "[35,   200] loss: 0.222\n",
      "[35,   250] loss: 0.229\n",
      "Accuracy of the network on the 1,200 validation images: 78.667 %\n",
      "[36,    50] loss: 0.181\n",
      "[36,   100] loss: 0.180\n",
      "[36,   150] loss: 0.189\n",
      "[36,   200] loss: 0.196\n",
      "[36,   250] loss: 0.194\n",
      "Accuracy of the network on the 1,200 validation images: 77.667 %\n",
      "[37,    50] loss: 0.173\n",
      "[37,   100] loss: 0.165\n",
      "[37,   150] loss: 0.177\n",
      "[37,   200] loss: 0.181\n",
      "[37,   250] loss: 0.192\n",
      "Accuracy of the network on the 1,200 validation images: 78.444 %\n",
      "[38,    50] loss: 0.161\n",
      "[38,   100] loss: 0.177\n",
      "[38,   150] loss: 0.180\n",
      "[38,   200] loss: 0.186\n",
      "[38,   250] loss: 0.190\n",
      "Accuracy of the network on the 1,200 validation images: 79.000 %\n",
      "[39,    50] loss: 0.165\n",
      "[39,   100] loss: 0.173\n",
      "[39,   150] loss: 0.181\n",
      "[39,   200] loss: 0.178\n",
      "[39,   250] loss: 0.192\n",
      "Accuracy of the network on the 1,200 validation images: 78.111 %\n",
      "[40,    50] loss: 0.153\n",
      "[40,   100] loss: 0.160\n",
      "[40,   150] loss: 0.170\n",
      "[40,   200] loss: 0.179\n",
      "[40,   250] loss: 0.175\n",
      "Accuracy of the network on the 1,200 validation images: 78.556 %\n",
      "[41,    50] loss: 0.146\n",
      "[41,   100] loss: 0.156\n",
      "[41,   150] loss: 0.155\n",
      "[41,   200] loss: 0.152\n",
      "[41,   250] loss: 0.161\n",
      "Accuracy of the network on the 1,200 validation images: 76.556 %\n",
      "[42,    50] loss: 0.142\n",
      "[42,   100] loss: 0.140\n",
      "[42,   150] loss: 0.151\n",
      "[42,   200] loss: 0.153\n",
      "[42,   250] loss: 0.153\n",
      "Accuracy of the network on the 1,200 validation images: 77.556 %\n",
      "[43,    50] loss: 0.132\n",
      "[43,   100] loss: 0.134\n",
      "[43,   150] loss: 0.141\n",
      "[43,   200] loss: 0.148\n",
      "[43,   250] loss: 0.155\n",
      "Accuracy of the network on the 1,200 validation images: 78.333 %\n",
      "[44,    50] loss: 0.116\n",
      "[44,   100] loss: 0.125\n",
      "[44,   150] loss: 0.138\n",
      "[44,   200] loss: 0.150\n",
      "[44,   250] loss: 0.166\n",
      "Accuracy of the network on the 1,200 validation images: 78.222 %\n",
      "[45,    50] loss: 0.127\n",
      "[45,   100] loss: 0.130\n",
      "[45,   150] loss: 0.130\n",
      "[45,   200] loss: 0.137\n",
      "[45,   250] loss: 0.152\n",
      "Accuracy of the network on the 1,200 validation images: 77.889 %\n",
      "[46,    50] loss: 0.126\n",
      "[46,   100] loss: 0.119\n",
      "[46,   150] loss: 0.127\n",
      "[46,   200] loss: 0.134\n",
      "[46,   250] loss: 0.130\n",
      "Accuracy of the network on the 1,200 validation images: 78.000 %\n",
      "[47,    50] loss: 0.112\n",
      "[47,   100] loss: 0.114\n",
      "[47,   150] loss: 0.116\n",
      "[47,   200] loss: 0.122\n",
      "[47,   250] loss: 0.124\n",
      "Accuracy of the network on the 1,200 validation images: 77.111 %\n",
      "[48,    50] loss: 0.104\n",
      "[48,   100] loss: 0.104\n",
      "[48,   150] loss: 0.116\n",
      "[48,   200] loss: 0.126\n",
      "[48,   250] loss: 0.130\n",
      "Accuracy of the network on the 1,200 validation images: 76.889 %\n",
      "[49,    50] loss: 0.096\n",
      "[49,   100] loss: 0.109\n",
      "[49,   150] loss: 0.113\n",
      "[49,   200] loss: 0.116\n",
      "[49,   250] loss: 0.117\n",
      "Accuracy of the network on the 1,200 validation images: 78.333 %\n",
      "[50,    50] loss: 0.103\n",
      "[50,   100] loss: 0.114\n",
      "[50,   150] loss: 0.112\n",
      "[50,   200] loss: 0.114\n",
      "[50,   250] loss: 0.116\n",
      "Accuracy of the network on the 1,200 validation images: 77.556 %\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "print(len(trainloader))\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 50 iterations\n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 50))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    lr_sche.step()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 1,200 validation images: %.3f %%' % (100 * correct / total))\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8d0a6czW_CC"
   },
   "source": [
    "# 💡4️⃣ [Model Evaluation]💡\n",
    "* Validation Set을 이용한 성능 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cPTYGtE4XNo3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1,200 validation images: 6.667 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in valloader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 1,200 validation images: %.3f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48qACLARmBSt"
   },
   "source": [
    "# 💡5️⃣ [Save Model]💡\n",
    "1. 아래 코드를 동작하면 Google 인증 팝업이 뜨고 인증을 요청.\n",
    "2. 인증후, Drive에 Final_Project/Model이라는 폴더가 생성됨.\n",
    "3. 생성된 폴더 안에 **model_{team_idx}.pt** 라는 파일이 생성됨.\n",
    "4. 생성된 파일과 코드를 다운받아 LMS에 제출\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eS87WmNpjgKa"
   },
   "outputs": [],
   "source": [
    "fname = f\"model_{team_idx}.pt\"\n",
    "\n",
    "model.eval()\n",
    "example_input = torch.randn(1,3,48,48).to(device)\n",
    "traced_script = torch.jit.trace(model, example_input)\n",
    "traced_script.save(fname)\n",
    "\n",
    "\n",
    "# 구축한 데이터셋 Google Drive로 저장\n",
    "# #from google.colab import drive\n",
    "# import shutil\n",
    "\n",
    "# # 1) Drive 마운트\n",
    "# #drive.mount('/content/drive')\n",
    "# data_dir = './content'\n",
    "# out_dirs = './content/Final_Project/model'\n",
    "# os.makedirs(out_dirs, exist_ok=True)\n",
    "\n",
    "# shutil.copy(os.path.join(data_dir, fname), os.path.join(out_dirs, fname))\n",
    "# print(\"Copied to Drive → MyDrive\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
