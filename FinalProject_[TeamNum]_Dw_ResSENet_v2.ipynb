{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojW8pWR2dJgi"
   },
   "source": [
    "# **üí°0Ô∏è‚É£ [ ÏïÑÎûò ÏΩîÎìúÎ•º Ïã§ÌñâÌïòÏó¨ Ï°∞Î≥Ñ Î≤àÌò∏Î•º ÏûÖÎ†•ÌïòÏãúÏò§.]üí°**\n",
    "\n",
    "*   1Ï°∞: Î∞ïÏú†ÏßÑ, ÍπÄÏÑúÏó∞, ÏµúÏßÑ\n",
    "*   2Ï°∞: ÏÑúÎØºÍ≤Ω, Ïù¥Ïú†ÏßÑ, ÏµúÎ≤îÏòÅ\n",
    "*   3Ï°∞: Ï†ïÏùÄÏ£º, ÍπÄÏû•Ìôò, Í∂åÏßÑÍ≤Ω\n",
    "*   4Ï°∞: Ï†ïÏßÑÍµê, Ïû•Ï±ÑÏùÄ, Ï°∞ÏÑ±Ìôò\n",
    "*   5Ï°∞: Ï†ÑÏäπÏû¨, Ïã†ÏùÄÌò∏, ÍπÄÏú§Ìù¨\n",
    "*   6Ï°∞: Ï†ïÌòÑÏÑú, ÏßÑÍ≤ΩÏùÄ, Ïù¥Ï†ïÎØº\n",
    "*   7Ï°∞: Ïù¥ÏÑ±ÏßÄ, ÍπÄÎÇ¥Í≤Ω\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_aMFjTcbntUh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ïòà]Ï°∞Ïùò Î≤àÌò∏Î•º ÏûÖÎ†•ÌïòÏãúÏò§: 1\n",
      ">> 5Ï°∞ ÏûÖÎãàÎã§.\n"
     ]
    }
   ],
   "source": [
    "print(\"[Ïòà]Ï°∞Ïùò Î≤àÌò∏Î•º ÏûÖÎ†•ÌïòÏãúÏò§: 1\")\n",
    "team_idx = int(input(\">> Ï°∞Ïùò Î≤àÌò∏Î•º ÏûÖÎ†•ÌïòÏãúÏò§: \"))\n",
    "print(f\">> {team_idx}Ï°∞ ÏûÖÎãàÎã§.\")\n",
    "team_idx = 'team'+str(team_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ltFiLYoDzdu9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.datasets.utils import download_url, extract_archive\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "from torchinfo import summary\n",
    "\n",
    "URL = \"https://github.com/JanghunHyeon/AISW4202-Project/releases/download/v.1.1.0/project_dataset.zip\"\n",
    "ROOT = \"./content/data\"\n",
    "ZIP_PATH = os.path.join(ROOT, \"project_dataset.zip\")\n",
    "OUT_DIR  = os.path.join(ROOT, \"project_dataset\")\n",
    "\n",
    "os.makedirs(ROOT, exist_ok=True)\n",
    "download_url(URL, root=ROOT, filename=\"project_dataset.zip\")\n",
    "extract_archive(ZIP_PATH, OUT_DIR)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# ReproduceÎ•º ÏúÑÌïú Seed Í≥†Ï†ï\n",
    "seed_id = 777\n",
    "deterministic = True\n",
    "\n",
    "random.seed(seed_id)\n",
    "np.random.seed(seed_id)\n",
    "torch.manual_seed(seed_id)\n",
    "if device =='cuda':\n",
    "    torch.cuda.manual_seed_all(seed_id)\n",
    "if deterministic:\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\ttorch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owZuDayDQfCI"
   },
   "source": [
    "# **üí°1Ô∏è‚É£ [ Design your custom model ]üí°**\n",
    "  ## ÏïÑÎûò ÏΩîÎìúÎ•º ÏàòÏ†ïÌïòÏó¨ Î≥∏Ïù∏Ïùò Î™®Îç∏ÏùÑ ÎßåÎì§Ïñ¥ Î≥¥ÏÑ∏Ïöî."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kA09kjRQRJm_"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 1. Î∏îÎ°ù Ï†ïÏùò\n",
    "############################\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, c_in, c_out, stride=1):\n",
    "        super().__init__()\n",
    "        self.depth = nn.Conv2d(c_in, c_in, 3, stride, 1, groups=c_in, bias=False)\n",
    "        self.point = nn.Conv2d(c_in, c_out, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c_out)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn(self.point(self.depth(x))))\n",
    "        return x\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, ch, r=4):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(ch, ch // r, bias=False)\n",
    "        self.fc2 = nn.Linear(ch // r, ch, bias=False)\n",
    "        self.act = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = x.mean((2,3))                    # Squeeze\n",
    "        s = self.sig(self.fc2(self.act(self.fc1(s))))\n",
    "        s = s.view(s.size(0), -1, 1, 1)\n",
    "        return x * s                         # Excitation\n",
    "\n",
    "class DwResSEBlock(nn.Module):\n",
    "    def __init__(self, c_in, c_out, stride):\n",
    "        super().__init__()\n",
    "        self.conv = DepthwiseSeparableConv(c_in, c_out, stride)\n",
    "        self.se   = SEBlock(c_out)\n",
    "        self.skip = nn.Sequential()\n",
    "        if stride != 1 or c_in != c_out:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(c_in, c_out, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(c_out))\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.se(self.conv(x))\n",
    "        out = out + self.skip(x)\n",
    "        return self.act(out)\n",
    "\n",
    "############################\n",
    "# 2. MyCustomModel ÍµêÏ≤¥\n",
    "############################\n",
    "class MyCustomModel(nn.Module):\n",
    "    def __init__(self, num_classes=15):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.stage1 = self._make_layer(32,  64,  2, stride=2) # 48‚Üí24\n",
    "        self.stage2 = self._make_layer(64,  128, 2, stride=2) # 24‚Üí12\n",
    "        self.stage3 = self._make_layer(128, 256, 3, stride=2) # 12‚Üí6\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc   = nn.Linear(256, num_classes)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, c_in, c_out, blocks, stride):\n",
    "        layers = [DwResSEBlock(c_in, c_out, stride)]\n",
    "        layers += [DwResSEBlock(c_out, c_out, 1) for _ in range(blocks-1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.pool(x).view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "HiQdYRtCRLtY"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "out_channels must be divisible by groups",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mMyCustomModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m      3\u001b[39m inputs = torch.Tensor(\u001b[32m1\u001b[39m,\u001b[32m3\u001b[39m,\u001b[32m48\u001b[39m,\u001b[32m48\u001b[39m).to(device)\n\u001b[32m      4\u001b[39m out = model(inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 265\u001b[39m, in \u001b[36mMyCustomModel.__init__\u001b[39m\u001b[34m(self, num_classes, init_weights)\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[38;5;28mself\u001b[39m.res_block2 = \u001b[38;5;28mself\u001b[39m._make_residual_block(\u001b[32m512\u001b[39m, \u001b[32m512\u001b[39m)\n\u001b[32m    263\u001b[39m \u001b[38;5;66;03m# Exit flow - Final feature extraction\u001b[39;00m\n\u001b[32m    264\u001b[39m \u001b[38;5;28mself\u001b[39m.exit_conv = nn.Sequential(\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     \u001b[43mAdvancedSeparableConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m728\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m    266\u001b[39m     nn.ReLU(),\n\u001b[32m    267\u001b[39m     AdvancedSeparableConv2d(\u001b[32m728\u001b[39m, \u001b[32m1024\u001b[39m, kernel_size=\u001b[32m3\u001b[39m, stride=\u001b[32m1\u001b[39m, padding=\u001b[32m1\u001b[39m),\n\u001b[32m    268\u001b[39m     nn.ReLU()\n\u001b[32m    269\u001b[39m )\n\u001b[32m    271\u001b[39m \u001b[38;5;66;03m# SE block for final features\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[38;5;28mself\u001b[39m.final_se = SEBlock(\u001b[32m1024\u001b[39m, reduction=\u001b[32m32\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mAdvancedSeparableConv2d.__init__\u001b[39m\u001b[34m(self, in_channels, out_channels, kernel_size, stride, padding, use_multiple_kernels, expansion_factor)\u001b[39m\n\u001b[32m     31\u001b[39m expanded_channels = in_channels * expansion_factor\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_multiple_kernels:\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Multi-scale depthwise convolution (like MobileNetV2 inspiration)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28mself\u001b[39m.depthwise_3x3 = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_channels\u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mself\u001b[39m.depthwise_5x5 = nn.Conv2d(in_channels, expanded_channels//\u001b[32m2\u001b[39m, kernel_size=\u001b[32m5\u001b[39m, \n\u001b[32m     38\u001b[39m                                  stride=stride, padding=\u001b[32m2\u001b[39m, groups=in_channels)\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mself\u001b[39m.bn_dw = nn.BatchNorm2d(expanded_channels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/pro/lib/python3.13/site-packages/torch/nn/modules/conv.py:521\u001b[39m, in \u001b[36mConv2d.__init__\u001b[39m\u001b[34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[39m\n\u001b[32m    519\u001b[39m padding_ = padding \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m _pair(padding)\n\u001b[32m    520\u001b[39m dilation_ = _pair(dilation)\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkernel_size_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdilation_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/pro/lib/python3.13/site-packages/torch/nn/modules/conv.py:109\u001b[39m, in \u001b[36m_ConvNd.__init__\u001b[39m\u001b[34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[39m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33min_channels must be divisible by groups\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out_channels % groups != \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mout_channels must be divisible by groups\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    110\u001b[39m valid_padding_strings = {\u001b[33m\"\u001b[39m\u001b[33msame\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalid\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[31mValueError\u001b[39m: out_channels must be divisible by groups"
     ]
    }
   ],
   "source": [
    "model = MyCustomModel(num_classes=15).to(device)\n",
    "\n",
    "inputs = torch.Tensor(1,3,48,48).to(device)\n",
    "out = model(inputs)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "iGFrmqopTjku"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "MyCustomModel                                 [1, 15]                   --\n",
       "‚îú‚îÄSequential: 1-1                             [1, 32, 48, 48]           --\n",
       "‚îÇ    ‚îî‚îÄConv2d: 2-1                            [1, 32, 48, 48]           864\n",
       "‚îÇ    ‚îî‚îÄBatchNorm2d: 2-2                       [1, 32, 48, 48]           64\n",
       "‚îÇ    ‚îî‚îÄReLU: 2-3                              [1, 32, 48, 48]           --\n",
       "‚îú‚îÄSequential: 1-2                             [1, 64, 24, 24]           --\n",
       "‚îÇ    ‚îî‚îÄDwResSEBlock: 2-4                      [1, 64, 24, 24]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDepthwiseSeparableConv: 3-1       [1, 64, 24, 24]           2,464\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSEBlock: 3-2                      [1, 64, 24, 24]           2,048\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-3                   [1, 64, 24, 24]           2,176\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-4                         [1, 64, 24, 24]           --\n",
       "‚îÇ    ‚îî‚îÄDwResSEBlock: 2-5                      [1, 64, 24, 24]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDepthwiseSeparableConv: 3-5       [1, 64, 24, 24]           4,800\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSEBlock: 3-6                      [1, 64, 24, 24]           2,048\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-7                   [1, 64, 24, 24]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-8                         [1, 64, 24, 24]           --\n",
       "‚îú‚îÄSequential: 1-3                             [1, 128, 12, 12]          --\n",
       "‚îÇ    ‚îî‚îÄDwResSEBlock: 2-6                      [1, 128, 12, 12]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDepthwiseSeparableConv: 3-9       [1, 128, 12, 12]          9,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSEBlock: 3-10                     [1, 128, 12, 12]          8,192\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-11                  [1, 128, 12, 12]          8,448\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-12                        [1, 128, 12, 12]          --\n",
       "‚îÇ    ‚îî‚îÄDwResSEBlock: 2-7                      [1, 128, 12, 12]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDepthwiseSeparableConv: 3-13      [1, 128, 12, 12]          17,792\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSEBlock: 3-14                     [1, 128, 12, 12]          8,192\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-15                  [1, 128, 12, 12]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-16                        [1, 128, 12, 12]          --\n",
       "‚îú‚îÄSequential: 1-4                             [1, 256, 6, 6]            --\n",
       "‚îÇ    ‚îî‚îÄDwResSEBlock: 2-8                      [1, 256, 6, 6]            --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDepthwiseSeparableConv: 3-17      [1, 256, 6, 6]            34,432\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSEBlock: 3-18                     [1, 256, 6, 6]            32,768\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-19                  [1, 256, 6, 6]            33,280\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-20                        [1, 256, 6, 6]            --\n",
       "‚îÇ    ‚îî‚îÄDwResSEBlock: 2-9                      [1, 256, 6, 6]            --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDepthwiseSeparableConv: 3-21      [1, 256, 6, 6]            68,352\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSEBlock: 3-22                     [1, 256, 6, 6]            32,768\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-23                  [1, 256, 6, 6]            --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-24                        [1, 256, 6, 6]            --\n",
       "‚îÇ    ‚îî‚îÄDwResSEBlock: 2-10                     [1, 256, 6, 6]            --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDepthwiseSeparableConv: 3-25      [1, 256, 6, 6]            68,352\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSEBlock: 3-26                     [1, 256, 6, 6]            32,768\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-27                  [1, 256, 6, 6]            --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-28                        [1, 256, 6, 6]            --\n",
       "‚îú‚îÄAdaptiveAvgPool2d: 1-5                      [1, 256, 1, 1]            --\n",
       "‚îú‚îÄLinear: 1-6                                 [1, 15]                   3,855\n",
       "===============================================================================================\n",
       "Total params: 372,687\n",
       "Trainable params: 372,687\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 19.59\n",
       "===============================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 5.28\n",
       "Params size (MB): 1.49\n",
       "Estimated Total Size (MB): 6.80\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, (1, 3, 48, 48))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4870zoF_T52f"
   },
   "source": [
    "# **üí°2Ô∏è‚É£[ Dataset Processing ]üí°**\n",
    "\n",
    "## Dataset for Final Project\n",
    "- Large-scale RGB image dataset\n",
    "  - Training set: about 75,000 images\n",
    "  - Validation set: 900 images\n",
    "  \n",
    "- Dataset specification\n",
    "  - image size: 48 x 48\n",
    "  - RGB scale image (3 channel)\n",
    "  - 15 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "53C2-8_TTqOG"
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std =[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std =[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "data_dir = os.path.join(OUT_DIR, \"data/ProjectDataset\")\n",
    "trainset = ImageFolder(os.path.join(data_dir, \"train\"), transform=transform_train)\n",
    "valset = ImageFolder(os.path.join(data_dir, \"val\"), transform=transform_val)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=256, shuffle=True, num_workers=6)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omABX5cXW67v"
   },
   "source": [
    "# **üí°3Ô∏è‚É£[ Train your custom model ]üí°**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "gH7IQXB8T_LH"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
    "\n",
    "lr_sche = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "oMb8Pr7TXBVH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n",
      "[1,    50] loss: 2.017\n",
      "[1,   100] loss: 1.676\n",
      "[1,   150] loss: 1.540\n",
      "[1,   200] loss: 1.429\n",
      "[1,   250] loss: 1.348\n",
      "Accuracy of the network on the 1,200 validation images: 6.667 %\n",
      "[2,    50] loss: 1.211\n",
      "[2,   100] loss: 1.156\n",
      "[2,   150] loss: 1.127\n",
      "[2,   200] loss: 1.098\n",
      "[2,   250] loss: 1.055\n",
      "Accuracy of the network on the 1,200 validation images: 6.667 %\n",
      "[3,    50] loss: 0.995\n",
      "[3,   100] loss: 0.970\n",
      "[3,   150] loss: 0.961\n",
      "[3,   200] loss: 0.953\n",
      "[3,   250] loss: 0.938\n",
      "Accuracy of the network on the 1,200 validation images: 68.111 %\n",
      "[4,    50] loss: 0.898\n",
      "[4,   100] loss: 0.894\n",
      "[4,   150] loss: 0.877\n",
      "[4,   200] loss: 0.878\n",
      "[4,   250] loss: 0.863\n",
      "Accuracy of the network on the 1,200 validation images: 72.889 %\n",
      "[5,    50] loss: 0.832\n",
      "[5,   100] loss: 0.808\n",
      "[5,   150] loss: 0.801\n",
      "[5,   200] loss: 0.807\n",
      "[5,   250] loss: 0.805\n",
      "Accuracy of the network on the 1,200 validation images: 73.222 %\n",
      "[6,    50] loss: 0.749\n",
      "[6,   100] loss: 0.732\n",
      "[6,   150] loss: 0.747\n",
      "[6,   200] loss: 0.749\n",
      "[6,   250] loss: 0.758\n",
      "Accuracy of the network on the 1,200 validation images: 74.000 %\n",
      "[7,    50] loss: 0.703\n",
      "[7,   100] loss: 0.692\n",
      "[7,   150] loss: 0.716\n",
      "[7,   200] loss: 0.728\n",
      "[7,   250] loss: 0.718\n",
      "Accuracy of the network on the 1,200 validation images: 74.333 %\n",
      "[8,    50] loss: 0.659\n",
      "[8,   100] loss: 0.686\n",
      "[8,   150] loss: 0.673\n",
      "[8,   200] loss: 0.686\n",
      "[8,   250] loss: 0.683\n",
      "Accuracy of the network on the 1,200 validation images: 76.111 %\n",
      "[9,    50] loss: 0.651\n",
      "[9,   100] loss: 0.633\n",
      "[9,   150] loss: 0.665\n",
      "[9,   200] loss: 0.655\n",
      "[9,   250] loss: 0.662\n",
      "Accuracy of the network on the 1,200 validation images: 75.556 %\n",
      "[10,    50] loss: 0.609\n",
      "[10,   100] loss: 0.614\n",
      "[10,   150] loss: 0.610\n",
      "[10,   200] loss: 0.622\n",
      "[10,   250] loss: 0.639\n",
      "Accuracy of the network on the 1,200 validation images: 76.778 %\n",
      "[11,    50] loss: 0.573\n",
      "[11,   100] loss: 0.582\n",
      "[11,   150] loss: 0.580\n",
      "[11,   200] loss: 0.591\n",
      "[11,   250] loss: 0.598\n",
      "Accuracy of the network on the 1,200 validation images: 76.111 %\n",
      "[12,    50] loss: 0.538\n",
      "[12,   100] loss: 0.556\n",
      "[12,   150] loss: 0.579\n",
      "[12,   200] loss: 0.576\n",
      "[12,   250] loss: 0.582\n",
      "Accuracy of the network on the 1,200 validation images: 77.444 %\n",
      "[13,    50] loss: 0.545\n",
      "[13,   100] loss: 0.518\n",
      "[13,   150] loss: 0.552\n",
      "[13,   200] loss: 0.553\n",
      "[13,   250] loss: 0.561\n",
      "Accuracy of the network on the 1,200 validation images: 77.000 %\n",
      "[14,    50] loss: 0.505\n",
      "[14,   100] loss: 0.523\n",
      "[14,   150] loss: 0.538\n",
      "[14,   200] loss: 0.528\n",
      "[14,   250] loss: 0.531\n",
      "Accuracy of the network on the 1,200 validation images: 77.444 %\n",
      "[15,    50] loss: 0.486\n",
      "[15,   100] loss: 0.498\n",
      "[15,   150] loss: 0.518\n",
      "[15,   200] loss: 0.494\n",
      "[15,   250] loss: 0.523\n",
      "Accuracy of the network on the 1,200 validation images: 78.111 %\n",
      "[16,    50] loss: 0.451\n",
      "[16,   100] loss: 0.463\n",
      "[16,   150] loss: 0.475\n",
      "[16,   200] loss: 0.485\n",
      "[16,   250] loss: 0.477\n",
      "Accuracy of the network on the 1,200 validation images: 77.111 %\n",
      "[17,    50] loss: 0.447\n",
      "[17,   100] loss: 0.443\n",
      "[17,   150] loss: 0.448\n",
      "[17,   200] loss: 0.458\n",
      "[17,   250] loss: 0.479\n",
      "Accuracy of the network on the 1,200 validation images: 78.778 %\n",
      "[18,    50] loss: 0.430\n",
      "[18,   100] loss: 0.420\n",
      "[18,   150] loss: 0.451\n",
      "[18,   200] loss: 0.443\n",
      "[18,   250] loss: 0.447\n",
      "Accuracy of the network on the 1,200 validation images: 79.444 %\n",
      "[19,    50] loss: 0.409\n",
      "[19,   100] loss: 0.415\n",
      "[19,   150] loss: 0.422\n",
      "[19,   200] loss: 0.435\n",
      "[19,   250] loss: 0.446\n",
      "Accuracy of the network on the 1,200 validation images: 79.556 %\n",
      "[20,    50] loss: 0.369\n",
      "[20,   100] loss: 0.415\n",
      "[20,   150] loss: 0.399\n",
      "[20,   200] loss: 0.420\n",
      "[20,   250] loss: 0.434\n",
      "Accuracy of the network on the 1,200 validation images: 78.111 %\n",
      "[21,    50] loss: 0.358\n",
      "[21,   100] loss: 0.369\n",
      "[21,   150] loss: 0.379\n",
      "[21,   200] loss: 0.402\n",
      "[21,   250] loss: 0.381\n",
      "Accuracy of the network on the 1,200 validation images: 79.333 %\n",
      "[22,    50] loss: 0.343\n",
      "[22,   100] loss: 0.364\n",
      "[22,   150] loss: 0.375\n",
      "[22,   200] loss: 0.367\n",
      "[22,   250] loss: 0.383\n",
      "Accuracy of the network on the 1,200 validation images: 79.444 %\n",
      "[23,    50] loss: 0.323\n",
      "[23,   100] loss: 0.351\n",
      "[23,   150] loss: 0.341\n",
      "[23,   200] loss: 0.364\n",
      "[23,   250] loss: 0.366\n",
      "Accuracy of the network on the 1,200 validation images: 78.889 %\n",
      "[24,    50] loss: 0.317\n",
      "[24,   100] loss: 0.319\n",
      "[24,   150] loss: 0.338\n",
      "[24,   200] loss: 0.350\n",
      "[24,   250] loss: 0.354\n",
      "Accuracy of the network on the 1,200 validation images: 78.556 %\n",
      "[25,    50] loss: 0.309\n",
      "[25,   100] loss: 0.321\n",
      "[25,   150] loss: 0.332\n",
      "[25,   200] loss: 0.337\n",
      "[25,   250] loss: 0.355\n",
      "Accuracy of the network on the 1,200 validation images: 77.667 %\n",
      "[26,    50] loss: 0.288\n",
      "[26,   100] loss: 0.286\n",
      "[26,   150] loss: 0.297\n",
      "[26,   200] loss: 0.303\n",
      "[26,   250] loss: 0.322\n",
      "Accuracy of the network on the 1,200 validation images: 77.778 %\n",
      "[27,    50] loss: 0.276\n",
      "[27,   100] loss: 0.272\n",
      "[27,   150] loss: 0.278\n",
      "[27,   200] loss: 0.293\n",
      "[27,   250] loss: 0.305\n",
      "Accuracy of the network on the 1,200 validation images: 77.556 %\n",
      "[28,    50] loss: 0.266\n",
      "[28,   100] loss: 0.274\n",
      "[28,   150] loss: 0.287\n",
      "[28,   200] loss: 0.279\n",
      "[28,   250] loss: 0.298\n",
      "Accuracy of the network on the 1,200 validation images: 78.333 %\n",
      "[29,    50] loss: 0.266\n",
      "[29,   100] loss: 0.251\n",
      "[29,   150] loss: 0.270\n",
      "[29,   200] loss: 0.269\n",
      "[29,   250] loss: 0.281\n",
      "Accuracy of the network on the 1,200 validation images: 78.111 %\n",
      "[30,    50] loss: 0.237\n",
      "[30,   100] loss: 0.237\n",
      "[30,   150] loss: 0.262\n",
      "[30,   200] loss: 0.273\n",
      "[30,   250] loss: 0.267\n",
      "Accuracy of the network on the 1,200 validation images: 78.444 %\n",
      "[31,    50] loss: 0.231\n",
      "[31,   100] loss: 0.228\n",
      "[31,   150] loss: 0.244\n",
      "[31,   200] loss: 0.235\n",
      "[31,   250] loss: 0.255\n",
      "Accuracy of the network on the 1,200 validation images: 77.222 %\n",
      "[32,    50] loss: 0.217\n",
      "[32,   100] loss: 0.224\n",
      "[32,   150] loss: 0.230\n",
      "[32,   200] loss: 0.233\n",
      "[32,   250] loss: 0.234\n",
      "Accuracy of the network on the 1,200 validation images: 77.444 %\n",
      "[33,    50] loss: 0.204\n",
      "[33,   100] loss: 0.209\n",
      "[33,   150] loss: 0.214\n",
      "[33,   200] loss: 0.248\n",
      "[33,   250] loss: 0.245\n",
      "Accuracy of the network on the 1,200 validation images: 78.222 %\n",
      "[34,    50] loss: 0.209\n",
      "[34,   100] loss: 0.194\n",
      "[34,   150] loss: 0.204\n",
      "[34,   200] loss: 0.217\n",
      "[34,   250] loss: 0.213\n",
      "Accuracy of the network on the 1,200 validation images: 77.556 %\n",
      "[35,    50] loss: 0.203\n",
      "[35,   100] loss: 0.191\n",
      "[35,   150] loss: 0.204\n",
      "[35,   200] loss: 0.222\n",
      "[35,   250] loss: 0.229\n",
      "Accuracy of the network on the 1,200 validation images: 78.667 %\n",
      "[36,    50] loss: 0.181\n",
      "[36,   100] loss: 0.180\n",
      "[36,   150] loss: 0.189\n",
      "[36,   200] loss: 0.196\n",
      "[36,   250] loss: 0.194\n",
      "Accuracy of the network on the 1,200 validation images: 77.667 %\n",
      "[37,    50] loss: 0.173\n",
      "[37,   100] loss: 0.165\n",
      "[37,   150] loss: 0.177\n",
      "[37,   200] loss: 0.181\n",
      "[37,   250] loss: 0.192\n",
      "Accuracy of the network on the 1,200 validation images: 78.444 %\n",
      "[38,    50] loss: 0.161\n",
      "[38,   100] loss: 0.177\n",
      "[38,   150] loss: 0.180\n",
      "[38,   200] loss: 0.186\n",
      "[38,   250] loss: 0.190\n",
      "Accuracy of the network on the 1,200 validation images: 79.000 %\n",
      "[39,    50] loss: 0.165\n",
      "[39,   100] loss: 0.173\n",
      "[39,   150] loss: 0.181\n",
      "[39,   200] loss: 0.178\n",
      "[39,   250] loss: 0.192\n",
      "Accuracy of the network on the 1,200 validation images: 78.111 %\n",
      "[40,    50] loss: 0.153\n",
      "[40,   100] loss: 0.160\n",
      "[40,   150] loss: 0.170\n",
      "[40,   200] loss: 0.179\n",
      "[40,   250] loss: 0.175\n",
      "Accuracy of the network on the 1,200 validation images: 78.556 %\n",
      "[41,    50] loss: 0.146\n",
      "[41,   100] loss: 0.156\n",
      "[41,   150] loss: 0.155\n",
      "[41,   200] loss: 0.152\n",
      "[41,   250] loss: 0.161\n",
      "Accuracy of the network on the 1,200 validation images: 76.556 %\n",
      "[42,    50] loss: 0.142\n",
      "[42,   100] loss: 0.140\n",
      "[42,   150] loss: 0.151\n",
      "[42,   200] loss: 0.153\n",
      "[42,   250] loss: 0.153\n",
      "Accuracy of the network on the 1,200 validation images: 77.556 %\n",
      "[43,    50] loss: 0.132\n",
      "[43,   100] loss: 0.134\n",
      "[43,   150] loss: 0.141\n",
      "[43,   200] loss: 0.148\n",
      "[43,   250] loss: 0.155\n",
      "Accuracy of the network on the 1,200 validation images: 78.333 %\n",
      "[44,    50] loss: 0.116\n",
      "[44,   100] loss: 0.125\n",
      "[44,   150] loss: 0.138\n",
      "[44,   200] loss: 0.150\n",
      "[44,   250] loss: 0.166\n",
      "Accuracy of the network on the 1,200 validation images: 78.222 %\n",
      "[45,    50] loss: 0.127\n",
      "[45,   100] loss: 0.130\n",
      "[45,   150] loss: 0.130\n",
      "[45,   200] loss: 0.137\n",
      "[45,   250] loss: 0.152\n",
      "Accuracy of the network on the 1,200 validation images: 77.889 %\n",
      "[46,    50] loss: 0.126\n",
      "[46,   100] loss: 0.119\n",
      "[46,   150] loss: 0.127\n",
      "[46,   200] loss: 0.134\n",
      "[46,   250] loss: 0.130\n",
      "Accuracy of the network on the 1,200 validation images: 78.000 %\n",
      "[47,    50] loss: 0.112\n",
      "[47,   100] loss: 0.114\n",
      "[47,   150] loss: 0.116\n",
      "[47,   200] loss: 0.122\n",
      "[47,   250] loss: 0.124\n",
      "Accuracy of the network on the 1,200 validation images: 77.111 %\n",
      "[48,    50] loss: 0.104\n",
      "[48,   100] loss: 0.104\n",
      "[48,   150] loss: 0.116\n",
      "[48,   200] loss: 0.126\n",
      "[48,   250] loss: 0.130\n",
      "Accuracy of the network on the 1,200 validation images: 76.889 %\n",
      "[49,    50] loss: 0.096\n",
      "[49,   100] loss: 0.109\n",
      "[49,   150] loss: 0.113\n",
      "[49,   200] loss: 0.116\n",
      "[49,   250] loss: 0.117\n",
      "Accuracy of the network on the 1,200 validation images: 78.333 %\n",
      "[50,    50] loss: 0.103\n",
      "[50,   100] loss: 0.114\n",
      "[50,   150] loss: 0.112\n",
      "[50,   200] loss: 0.114\n",
      "[50,   250] loss: 0.116\n",
      "Accuracy of the network on the 1,200 validation images: 77.556 %\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "print(len(trainloader))\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 50 iterations\n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 50))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    lr_sche.step()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 1,200 validation images: %.3f %%' % (100 * correct / total))\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8d0a6czW_CC"
   },
   "source": [
    "# üí°4Ô∏è‚É£ [Model Evaluation]üí°\n",
    "* Validation SetÏùÑ Ïù¥Ïö©Ìïú ÏÑ±Îä• Í≤ÄÏ¶ù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cPTYGtE4XNo3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1,200 validation images: 6.667 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in valloader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 1,200 validation images: %.3f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48qACLARmBSt"
   },
   "source": [
    "# üí°5Ô∏è‚É£ [Save Model]üí°\n",
    "1. ÏïÑÎûò ÏΩîÎìúÎ•º ÎèôÏûëÌïòÎ©¥ Google Ïù∏Ï¶ù ÌåùÏóÖÏù¥ Îú®Í≥† Ïù∏Ï¶ùÏùÑ ÏöîÏ≤≠.\n",
    "2. Ïù∏Ï¶ùÌõÑ, DriveÏóê Final_Project/ModelÏù¥ÎùºÎäî Ìè¥ÎçîÍ∞Ä ÏÉùÏÑ±Îê®.\n",
    "3. ÏÉùÏÑ±Îêú Ìè¥Îçî ÏïàÏóê **model_{team_idx}.pt** ÎùºÎäî ÌååÏùºÏù¥ ÏÉùÏÑ±Îê®.\n",
    "4. ÏÉùÏÑ±Îêú ÌååÏùºÍ≥º ÏΩîÎìúÎ•º Îã§Ïö¥Î∞õÏïÑ LMSÏóê Ï†úÏ∂ú\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eS87WmNpjgKa"
   },
   "outputs": [],
   "source": [
    "fname = f\"model_{team_idx}.pt\"\n",
    "\n",
    "model.eval()\n",
    "example_input = torch.randn(1,3,48,48).to(device)\n",
    "traced_script = torch.jit.trace(model, example_input)\n",
    "traced_script.save(fname)\n",
    "\n",
    "\n",
    "# Íµ¨Ï∂ïÌïú Îç∞Ïù¥ÌÑ∞ÏÖã Google DriveÎ°ú Ï†ÄÏû•\n",
    "# #from google.colab import drive\n",
    "# import shutil\n",
    "\n",
    "# # 1) Drive ÎßàÏö¥Ìä∏\n",
    "# #drive.mount('/content/drive')\n",
    "# data_dir = './content'\n",
    "# out_dirs = './content/Final_Project/model'\n",
    "# os.makedirs(out_dirs, exist_ok=True)\n",
    "\n",
    "# shutil.copy(os.path.join(data_dir, fname), os.path.join(out_dirs, fname))\n",
    "# print(\"Copied to Drive ‚Üí MyDrive\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
