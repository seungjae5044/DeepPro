{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojW8pWR2dJgi"
   },
   "source": [
    "# **💡0️⃣ [ 아래 코드를 실행하여 조별 번호를 입력하시오.]💡**\n",
    "\n",
    "*   1조: 박유진, 김서연, 최진\n",
    "*   2조: 서민경, 이유진, 최범영\n",
    "*   3조: 정은주, 김장환, 권진경\n",
    "*   4조: 정진교, 장채은, 조성환\n",
    "*   5조: 전승재, 신은호, 김윤희\n",
    "*   6조: 정현서, 진경은, 이정민\n",
    "*   7조: 이성지, 김내경\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_aMFjTcbntUh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[예]조의 번호를 입력하시오: 1\n",
      ">> 5조 입니다.\n"
     ]
    }
   ],
   "source": [
    "print(\"[예]조의 번호를 입력하시오: 1\")\n",
    "team_idx = int(input(\">> 조의 번호를 입력하시오: \"))\n",
    "print(f\">> {team_idx}조 입니다.\")\n",
    "team_idx = 'team'+str(team_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ltFiLYoDzdu9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.datasets.utils import download_url, extract_archive\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "from torchinfo import summary\n",
    "\n",
    "URL = \"https://github.com/JanghunHyeon/AISW4202-Project/releases/download/v.1.1.0/project_dataset.zip\"\n",
    "ROOT = \"./content/data\"\n",
    "ZIP_PATH = os.path.join(ROOT, \"project_dataset.zip\")\n",
    "OUT_DIR  = os.path.join(ROOT, \"project_dataset\")\n",
    "\n",
    "os.makedirs(ROOT, exist_ok=True)\n",
    "download_url(URL, root=ROOT, filename=\"project_dataset.zip\")\n",
    "extract_archive(ZIP_PATH, OUT_DIR)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# Reproduce를 위한 Seed 고정\n",
    "seed_id = 777\n",
    "deterministic = True\n",
    "\n",
    "random.seed(seed_id)\n",
    "np.random.seed(seed_id)\n",
    "torch.manual_seed(seed_id)\n",
    "if device =='cuda':\n",
    "    torch.cuda.manual_seed_all(seed_id)\n",
    "if deterministic:\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\ttorch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owZuDayDQfCI"
   },
   "source": [
    "# **💡1️⃣ [ Design your custom model ]💡**\n",
    "  ## 아래 코드를 수정하여 본인의 모델을 만들어 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kA09kjRQRJm_"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 1. 블록 정의\n",
    "############################\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, c_in, c_out, stride=1):\n",
    "        super().__init__()\n",
    "        self.depth = nn.Conv2d(c_in, c_in, 3, stride, 1, groups=c_in, bias=False)\n",
    "        self.point = nn.Conv2d(c_in, c_out, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c_out)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn(self.point(self.depth(x))))\n",
    "        return x\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, ch, r=4):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(ch, ch // r, bias=False)\n",
    "        self.fc2 = nn.Linear(ch // r, ch, bias=False)\n",
    "        self.act = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = x.mean((2,3))                    # Squeeze\n",
    "        s = self.sig(self.fc2(self.act(self.fc1(s))))\n",
    "        s = s.view(s.size(0), -1, 1, 1)\n",
    "        return x * s                         # Excitation\n",
    "\n",
    "class DwResSEBlock(nn.Module):\n",
    "    def __init__(self, c_in, c_out, stride):\n",
    "        super().__init__()\n",
    "        self.conv = DepthwiseSeparableConv(c_in, c_out, stride)\n",
    "        self.se   = SEBlock(c_out)\n",
    "        self.skip = nn.Sequential()\n",
    "        if stride != 1 or c_in != c_out:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(c_in, c_out, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(c_out))\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.se(self.conv(x))\n",
    "        out = out + self.skip(x)\n",
    "        return self.act(out)\n",
    "\n",
    "############################\n",
    "# 2. MyCustomModel 교체\n",
    "############################\n",
    "class MyCustomModel(nn.Module):\n",
    "    def __init__(self, num_classes=15):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.stage1 = self._make_layer(32,  64,  2, stride=2) # 48→24\n",
    "        self.stage2 = self._make_layer(64,  128, 2, stride=2) # 24→12\n",
    "        self.stage3 = self._make_layer(128, 256, 3, stride=2) # 12→6\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc   = nn.Linear(512, num_classes)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, c_in, c_out, blocks, stride):\n",
    "        layers = [DwResSEBlock(c_in, c_out, stride)]\n",
    "        layers += [DwResSEBlock(c_out, c_out, 1) for _ in range(blocks-1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.pool(x).view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HiQdYRtCRLtY"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MyCustomModel' object has no attribute 'stage4'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m model = MyCustomModel(num_classes=\u001b[32m15\u001b[39m).to(device)\n\u001b[32m      3\u001b[39m inputs = torch.Tensor(\u001b[32m1\u001b[39m,\u001b[32m3\u001b[39m,\u001b[32m48\u001b[39m,\u001b[32m48\u001b[39m).to(device)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(out.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/pro/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/pro/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mMyCustomModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     75\u001b[39m x = \u001b[38;5;28mself\u001b[39m.stage2(x)\n\u001b[32m     76\u001b[39m x = \u001b[38;5;28mself\u001b[39m.stage3(x)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstage4\u001b[49m(x)\n\u001b[32m     78\u001b[39m x = \u001b[38;5;28mself\u001b[39m.pool(x).view(x.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fc(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/pro/lib/python3.13/site-packages/torch/nn/modules/module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'MyCustomModel' object has no attribute 'stage4'"
     ]
    }
   ],
   "source": [
    "model = MyCustomModel(num_classes=15).to(device)\n",
    "\n",
    "inputs = torch.Tensor(1,3,48,48).to(device)\n",
    "out = model(inputs)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.8659e-02,  3.0929e-02,  3.9412e-02, -2.2221e-02,  1.1541e-02,\n",
      "         -2.0074e-02, -3.9873e-02,  1.6007e-02,  3.4316e-02, -1.7259e-02,\n",
      "         -1.1037e-02,  2.2265e-02,  2.9167e-02, -7.4874e-05,  1.5028e-02]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iGFrmqopTjku"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "MyCustomModel                                 [1, 15]                   --\n",
       "├─Sequential: 1-1                             [1, 32, 48, 48]           --\n",
       "│    └─Conv2d: 2-1                            [1, 32, 48, 48]           864\n",
       "│    └─BatchNorm2d: 2-2                       [1, 32, 48, 48]           64\n",
       "│    └─ReLU: 2-3                              [1, 32, 48, 48]           --\n",
       "├─Sequential: 1-2                             [1, 64, 24, 24]           --\n",
       "│    └─DwResSEBlock: 2-4                      [1, 64, 24, 24]           --\n",
       "│    │    └─DepthwiseSeparableConv: 3-1       [1, 64, 24, 24]           2,464\n",
       "│    │    └─SEBlock: 3-2                      [1, 64, 24, 24]           2,048\n",
       "│    │    └─Sequential: 3-3                   [1, 64, 24, 24]           2,176\n",
       "│    │    └─ReLU: 3-4                         [1, 64, 24, 24]           --\n",
       "│    └─DwResSEBlock: 2-5                      [1, 64, 24, 24]           --\n",
       "│    │    └─DepthwiseSeparableConv: 3-5       [1, 64, 24, 24]           4,800\n",
       "│    │    └─SEBlock: 3-6                      [1, 64, 24, 24]           2,048\n",
       "│    │    └─Sequential: 3-7                   [1, 64, 24, 24]           --\n",
       "│    │    └─ReLU: 3-8                         [1, 64, 24, 24]           --\n",
       "├─Sequential: 1-3                             [1, 128, 12, 12]          --\n",
       "│    └─DwResSEBlock: 2-6                      [1, 128, 12, 12]          --\n",
       "│    │    └─DepthwiseSeparableConv: 3-9       [1, 128, 12, 12]          9,024\n",
       "│    │    └─SEBlock: 3-10                     [1, 128, 12, 12]          8,192\n",
       "│    │    └─Sequential: 3-11                  [1, 128, 12, 12]          8,448\n",
       "│    │    └─ReLU: 3-12                        [1, 128, 12, 12]          --\n",
       "│    └─DwResSEBlock: 2-7                      [1, 128, 12, 12]          --\n",
       "│    │    └─DepthwiseSeparableConv: 3-13      [1, 128, 12, 12]          17,792\n",
       "│    │    └─SEBlock: 3-14                     [1, 128, 12, 12]          8,192\n",
       "│    │    └─Sequential: 3-15                  [1, 128, 12, 12]          --\n",
       "│    │    └─ReLU: 3-16                        [1, 128, 12, 12]          --\n",
       "├─Sequential: 1-4                             [1, 256, 6, 6]            --\n",
       "│    └─DwResSEBlock: 2-8                      [1, 256, 6, 6]            --\n",
       "│    │    └─DepthwiseSeparableConv: 3-17      [1, 256, 6, 6]            34,432\n",
       "│    │    └─SEBlock: 3-18                     [1, 256, 6, 6]            32,768\n",
       "│    │    └─Sequential: 3-19                  [1, 256, 6, 6]            33,280\n",
       "│    │    └─ReLU: 3-20                        [1, 256, 6, 6]            --\n",
       "│    └─DwResSEBlock: 2-9                      [1, 256, 6, 6]            --\n",
       "│    │    └─DepthwiseSeparableConv: 3-21      [1, 256, 6, 6]            68,352\n",
       "│    │    └─SEBlock: 3-22                     [1, 256, 6, 6]            32,768\n",
       "│    │    └─Sequential: 3-23                  [1, 256, 6, 6]            --\n",
       "│    │    └─ReLU: 3-24                        [1, 256, 6, 6]            --\n",
       "│    └─DwResSEBlock: 2-10                     [1, 256, 6, 6]            --\n",
       "│    │    └─DepthwiseSeparableConv: 3-25      [1, 256, 6, 6]            68,352\n",
       "│    │    └─SEBlock: 3-26                     [1, 256, 6, 6]            32,768\n",
       "│    │    └─Sequential: 3-27                  [1, 256, 6, 6]            --\n",
       "│    │    └─ReLU: 3-28                        [1, 256, 6, 6]            --\n",
       "├─Sequential: 1-5                             [1, 512, 3, 3]            --\n",
       "│    └─DwResSEBlock: 2-11                     [1, 512, 3, 3]            --\n",
       "│    │    └─DepthwiseSeparableConv: 3-29      [1, 512, 3, 3]            134,400\n",
       "│    │    └─SEBlock: 3-30                     [1, 512, 3, 3]            131,072\n",
       "│    │    └─Sequential: 3-31                  [1, 512, 3, 3]            132,096\n",
       "│    │    └─ReLU: 3-32                        [1, 512, 3, 3]            --\n",
       "│    └─DwResSEBlock: 2-12                     [1, 512, 3, 3]            --\n",
       "│    │    └─DepthwiseSeparableConv: 3-33      [1, 512, 3, 3]            267,776\n",
       "│    │    └─SEBlock: 3-34                     [1, 512, 3, 3]            131,072\n",
       "│    │    └─Sequential: 3-35                  [1, 512, 3, 3]            --\n",
       "│    │    └─ReLU: 3-36                        [1, 512, 3, 3]            --\n",
       "├─AdaptiveAvgPool2d: 1-6                      [1, 512, 1, 1]            --\n",
       "├─Linear: 1-7                                 [1, 15]                   7,695\n",
       "===============================================================================================\n",
       "Total params: 1,172,943\n",
       "Trainable params: 1,172,943\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 24.64\n",
       "===============================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 5.57\n",
       "Params size (MB): 4.69\n",
       "Estimated Total Size (MB): 10.29\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, (1, 3, 48, 48))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4870zoF_T52f"
   },
   "source": [
    "# **💡2️⃣[ Dataset Processing ]💡**\n",
    "\n",
    "## Dataset for Final Project\n",
    "- Large-scale RGB image dataset\n",
    "  - Training set: about 75,000 images\n",
    "  - Validation set: 900 images\n",
    "  \n",
    "- Dataset specification\n",
    "  - image size: 48 x 48\n",
    "  - RGB scale image (3 channel)\n",
    "  - 15 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "53C2-8_TTqOG"
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std =[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std =[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "data_dir = os.path.join(OUT_DIR, \"data/ProjectDataset\")\n",
    "trainset = ImageFolder(os.path.join(data_dir, \"train\"), transform=transform_train)\n",
    "valset = ImageFolder(os.path.join(data_dir, \"val\"), transform=transform_val)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=256, shuffle=True, num_workers=6)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omABX5cXW67v"
   },
   "source": [
    "# **💡3️⃣[ Train your custom model ]💡**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gH7IQXB8T_LH"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
    "\n",
    "lr_sche = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "oMb8Pr7TXBVH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n",
      "[1,    50] loss: 2.058\n",
      "[1,   100] loss: 1.671\n",
      "[1,   150] loss: 1.505\n",
      "[1,   200] loss: 1.407\n",
      "[1,   250] loss: 1.317\n",
      "Accuracy of the network on the 1,200 validation images: 6.667 %\n",
      "[2,    50] loss: 1.184\n",
      "[2,   100] loss: 1.150\n",
      "[2,   150] loss: 1.134\n",
      "[2,   200] loss: 1.103\n",
      "[2,   250] loss: 1.065\n",
      "Accuracy of the network on the 1,200 validation images: 6.667 %\n",
      "[3,    50] loss: 0.987\n",
      "[3,   100] loss: 0.970\n",
      "[3,   150] loss: 0.950\n",
      "[3,   200] loss: 0.956\n",
      "[3,   250] loss: 0.930\n",
      "Accuracy of the network on the 1,200 validation images: 6.667 %\n",
      "[4,    50] loss: 0.846\n",
      "[4,   100] loss: 0.860\n",
      "[4,   150] loss: 0.846\n",
      "[4,   200] loss: 0.857\n",
      "[4,   250] loss: 0.859\n",
      "Accuracy of the network on the 1,200 validation images: 6.667 %\n",
      "[5,    50] loss: 0.793\n",
      "[5,   100] loss: 0.798\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m outputs = model(inputs)\n\u001b[32m     17\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m optimizer.step()\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# print statistics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/pro/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/pro/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/pro/lib/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(len(trainloader))\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 50 iterations\n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 50))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    lr_sche.step()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 1,200 validation images: %.3f %%' % (100 * correct / total))\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8d0a6czW_CC"
   },
   "source": [
    "# 💡4️⃣ [Model Evaluation]💡\n",
    "* Validation Set을 이용한 성능 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cPTYGtE4XNo3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1,200 validation images: 6.667 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in valloader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 1,200 validation images: %.3f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48qACLARmBSt"
   },
   "source": [
    "# 💡5️⃣ [Save Model]💡\n",
    "1. 아래 코드를 동작하면 Google 인증 팝업이 뜨고 인증을 요청.\n",
    "2. 인증후, Drive에 Final_Project/Model이라는 폴더가 생성됨.\n",
    "3. 생성된 폴더 안에 **model_{team_idx}.pt** 라는 파일이 생성됨.\n",
    "4. 생성된 파일과 코드를 다운받아 LMS에 제출\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eS87WmNpjgKa"
   },
   "outputs": [],
   "source": [
    "fname = f\"model_{team_idx}.pt\"\n",
    "\n",
    "model.eval()\n",
    "example_input = torch.randn(1,3,48,48).to(device)\n",
    "traced_script = torch.jit.trace(model, example_input)\n",
    "traced_script.save(fname)\n",
    "\n",
    "\n",
    "# 구축한 데이터셋 Google Drive로 저장\n",
    "# #from google.colab import drive\n",
    "# import shutil\n",
    "\n",
    "# # 1) Drive 마운트\n",
    "# #drive.mount('/content/drive')\n",
    "# data_dir = './content'\n",
    "# out_dirs = './content/Final_Project/model'\n",
    "# os.makedirs(out_dirs, exist_ok=True)\n",
    "\n",
    "# shutil.copy(os.path.join(data_dir, fname), os.path.join(out_dirs, fname))\n",
    "# print(\"Copied to Drive → MyDrive\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
